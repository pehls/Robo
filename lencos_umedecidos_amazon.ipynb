{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para instalação do Selenium:\n",
    "https://selenium-python.readthedocs.io/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para instalação do BeautifulSoup:\n",
    "https://pypi.org/project/beautifulsoup4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import ndjson\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from pandas.io.json import json_normalize\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "\n",
    "## Declaração das variáveis para envio ao bigquery\n",
    "key_path = \"C:/Robo/FinanceBOT.json\"\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "data_e_hora_atuais = datetime.now()\n",
    "data_e_hora_em_texto = data_e_hora_atuais.strftime('%d%m%Y_%H%M')\n",
    "os.chdir('C:/Robo/Dados')\n",
    "\n",
    "dados_totais = {'imgs': [], 'links':[], 'text':[], 'stars':[], 'price':[], 'economy':[]}\n",
    "dados_totais = pd.DataFrame(data=dados_totais)\n",
    "\n",
    "# Aqui, uso o webdriver do chrome\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# O send keys \"toma o controle\" do Chrome para envio de comandos\n",
    "body = browser.find_element_by_tag_name(\"body\")\n",
    "body.send_keys(Keys.CONTROL + 't')\n",
    "\n",
    "# O browser.get envia o site para acessar\n",
    "browser.get(\"https://www.amazon.com.br/ref=nav_logo\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Procuramos através do inspecionar a text box para pesquisar, e pegando o \"full path\" através do botão direito na aba de inspeção do botão para o search na amazon, com o evento de click\n",
    "search_box = browser.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"fralda RN\")\n",
    "browser.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "browser.find_element_by_class_name('a-button-dropdown').click()\n",
    "time.sleep(2)\n",
    "\n",
    "browser.find_element_by_id('s-result-sort-select_1').click()\n",
    "time.sleep(2)\n",
    "\n",
    "n_paginas = int(BeautifulSoup(browser.find_element_by_class_name('a-pagination').get_attribute(\"innerHTML\"), \"html.parser\").findAll(class_='a-disabled')[1].text)\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabri\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:36: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for execucoes in range(1,n_paginas):\n",
    "    lista_resultados = browser.find_element_by_class_name('s-main-slot')\n",
    "    time.sleep(2)\n",
    "    html = lista_resultados.get_attribute(\"innerHTML\")\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    time.sleep(2)\n",
    "    imgs = []\n",
    "    links = []\n",
    "    text = []\n",
    "    stars = []\n",
    "    price = []\n",
    "    economy = []\n",
    "    fretes = []   \n",
    "    ## No geral, uso as classes (class_=) ou então os elementos (como 'span', 'div') ou o próprio texto (como em economy abaixo)\n",
    "    for div in soup.findAll('div'):\n",
    "        index = (div.get('data-index'))\n",
    "        if (not(index is None)):\n",
    "            if(int(index) in range(0,24)):\n",
    "                imgs.append('' if (div.findAll(class_='s-image')== []) else (div.findAll(class_='s-image'))[0].get('src'))\n",
    "                links.append('' if (div.findAll(class_='a-link-normal')== []) else 'https://www.amazon.com.br/'+(div.findAll(class_='a-link-normal'))[0].get('href'))\n",
    "                text.append('' if (div.findAll(class_='a-text-normal') == []) else (div.findAll(class_='a-text-normal')[0].find('span').text))\n",
    "                stars.append('' if (div.findAll(class_='a-icon-alt') == []) else div.findAll(class_='a-icon-alt')[0].text)\n",
    "                price.append('' if (div.findAll(class_='a-price') == []) else (div.findAll(class_='a-price')[0].find('span').text))\n",
    "                economy.append(('' if (div.findAll(class_='a-color-secondary')) == [] else div.findAll(class_='a-color-secondary')[0].text if 'Economize' in div.findAll(class_='a-color-secondary')[0].text else ''))\n",
    "                frete = ''\n",
    "                for span in div.findAll('span'):\n",
    "                    aria_label = span.get('aria-label')\n",
    "                    if not(aria_label == None):\n",
    "                        if(\"frete\" in aria_label.lower()):\n",
    "                            frete = (aria_label)\n",
    "                fretes.append(frete)\n",
    "    ## No fim, crio um DF com os itens selecionados, concateno com o já existente\n",
    "    dados = {'imgs': imgs, 'links':links, 'text':text, 'stars':stars, 'price':price, 'economy':economy, 'frete':fretes}\n",
    "    dados = pd.DataFrame(data=dados)\n",
    "    dados_totais = pd.concat([dados,dados_totais], ignore_index=True)\n",
    "    time.sleep(2)\n",
    "    browser.find_element_by_class_name('a-last').click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## E abaixo, envio ao bigquery ou salvo em csv / comentado\n",
    "dados_totais.to_csv(\"C:/Robo/Dados/fraldas_amazon_\"+data_e_hora_em_texto+\".csv\", index=False)\n",
    "dados_totais.to_gbq(destination_table='priceanalysis.fraldas_'+data_e_hora_em_texto, if_exists='append', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
